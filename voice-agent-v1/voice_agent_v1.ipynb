{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\asus\\desktop\\alphablocks\\firday_projects\\generative_ai_learning_env\\lib\\site-packages (0.2.12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement langchain.memory (from versions: none)\n",
      "ERROR: No matching distribution found for langchain.memory\n",
      "WARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Asus\\Desktop\\alphablocks\\firday_projects\\generative_ai_learning_env\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain langchain.memory langchain_community openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.38.0-py3-none-any.whl (335 kB)\n",
      "Collecting anyio<5,>=3.5.0\n",
      "  Using cached anyio-4.4.0-py3-none-any.whl (86 kB)\n",
      "Collecting httpx<1,>=0.23.0\n",
      "  Using cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "Collecting distro<2,>=1.7.0\n",
      "  Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\asus\\desktop\\alphablocks\\firday_projects\\generative_ai_learning_env\\lib\\site-packages (from openai) (2.8.2)\n",
      "Collecting tqdm>4\n",
      "  Using cached tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "Collecting sniffio\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\asus\\desktop\\alphablocks\\firday_projects\\generative_ai_learning_env\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\asus\\desktop\\alphablocks\\firday_projects\\generative_ai_learning_env\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\asus\\desktop\\alphablocks\\firday_projects\\generative_ai_learning_env\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\asus\\desktop\\alphablocks\\firday_projects\\generative_ai_learning_env\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
      "Collecting httpcore==1.*\n",
      "  Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Collecting h11<0.15,>=0.13\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\asus\\desktop\\alphablocks\\firday_projects\\generative_ai_learning_env\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\asus\\desktop\\alphablocks\\firday_projects\\generative_ai_learning_env\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\desktop\\alphablocks\\firday_projects\\generative_ai_learning_env\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Installing collected packages: sniffio, h11, httpcore, anyio, tqdm, httpx, distro, openai\n",
      "Successfully installed anyio-4.4.0 distro-1.9.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.38.0 sniffio-1.3.1 tqdm-4.66.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Asus\\Desktop\\alphablocks\\firday_projects\\generative_ai_learning_env\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "! pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install SpeechRecognition pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "import pyttsx3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"sk-proj-uwPFhiizoTWB73V2QJ0iT3BlbkFJiahGtUsvQk35qiyPm0Zf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\Desktop\\alphablocks\\firday_projects\\generative_ai_learning_env\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain import OpenAI, ConversationChain, LLMChain, PromptTemplate\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "\n",
    "template = \"\"\"Assistant is a large language model trained by OpenAI.\n",
    "\n",
    "Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
    "\n",
    "Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
    "\n",
    "Overall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n",
    "\n",
    "Assistant is aware that human input is being transcribed from audio and as such there may be some errors in the transcription. It will attempt to account for some words being swapped with similar-sounding words or phrases. Assistant will also keep responses concise, because human attention spans are more limited over the audio channel since it takes time to listen to a response.\n",
    "\n",
    "{history}\n",
    "Human: {human_input}\n",
    "Assistant:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"history\", \"human_input\"], template=template)\n",
    "\n",
    "\n",
    "chatgpt_chain = LLMChain(\n",
    "    llm=OpenAI(temperature=0),\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferWindowMemory(k=2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyaudio in c:\\users\\asus\\desktop\\alphablocks\\firday_projects\\generative_ai_learning_env\\lib\\site-packages (0.2.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Asus\\Desktop\\alphablocks\\firday_projects\\generative_ai_learning_env\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyaudio\n",
    "%pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Asus\\Desktop\\alphablocks\\firday_projects\\generative_ai_learning_env\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.4.0-cp39-cp39-win_amd64.whl (198.0 MB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Collecting sympy\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\asus\\desktop\\alphablocks\\firday_projects\\generative_ai_learning_env\\lib\\site-packages (from torch) (4.12.2)\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Collecting fsspec\n",
      "  Using cached fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Collecting jinja2\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Using cached MarkupSafe-2.1.5-cp39-cp39-win_amd64.whl (17 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, MarkupSafe, sympy, networkx, jinja2, fsspec, filelock, torch\n",
      "Successfully installed MarkupSafe-2.1.5 filelock-3.15.4 fsspec-2024.6.1 jinja2-3.1.4 mpmath-1.3.0 networkx-3.2.1 sympy-1.13.1 torch-2.4.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.4.0-cp39-cp39-win_amd64.whl (198.0 MB)\n",
      "Collecting jinja2\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Collecting fsspec\n",
      "  Using cached fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Collecting networkx\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\asus\\desktop\\alphablocks\\firday_projects\\generative_ai_learning_env\\lib\\site-packages (from torch) (4.12.2)\n",
      "Collecting sympy\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Using cached MarkupSafe-2.1.5-cp39-cp39-win_amd64.whl (17 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, MarkupSafe, sympy, networkx, jinja2, fsspec, filelock, torch\n",
      "Successfully installed MarkupSafe-2.1.5 filelock-3.15.4 fsspec-2024.6.1 jinja2-3.1.4 mpmath-1.3.0 networkx-3.2.1 sympy-1.13.1 torch-2.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Asus\\Desktop\\alphablocks\\firday_projects\\generative_ai_learning_env\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "import pyttsx3\n",
    "\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "\n",
    "def listen():\n",
    "    r = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Calibrating...\")\n",
    "        r.adjust_for_ambient_noise(source, duration=5)\n",
    "        # optional parameters to adjust microphone sensitivity\n",
    "        # r.energy_threshold = 200\n",
    "        # r.pause_threshold=0.5\n",
    "\n",
    "        print(\"Okay, go!\")\n",
    "        while 1:\n",
    "            text = \"\"\n",
    "            print(\"listening now...\")\n",
    "            try:\n",
    "                audio = r.listen(source, timeout=5, phrase_time_limit=30)\n",
    "                print(\"Recognizing...\")\n",
    "                # whisper model options are found here: https://github.com/openai/whisper#available-models-and-languages\n",
    "                # other speech recognition models are also available.\n",
    "                text = r.recognize_whisper(\n",
    "                    audio,\n",
    "                    model=\"medium.en\",\n",
    "                    show_dict=True,\n",
    "                )[\"text\"]\n",
    "            except Exception as e:\n",
    "                unrecognized_speech_text = (\n",
    "                    f\"Sorry, I didn't catch that. Exception was: {e}s\"\n",
    "                )\n",
    "                text = unrecognized_speech_text\n",
    "            print(text)\n",
    "\n",
    "            response_text = chatgpt_chain.predict(human_input=text)\n",
    "            print(response_text)\n",
    "            engine.say(response_text)\n",
    "            engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting soundfile\n",
      "  Downloading soundfile-0.12.1-py2.py3-none-win_amd64.whl (1.0 MB)\n",
      "Collecting cffi>=1.0\n",
      "  Using cached cffi-1.16.0-cp39-cp39-win_amd64.whl (181 kB)\n",
      "Collecting pycparser\n",
      "  Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: pycparser, cffi, soundfile\n",
      "Successfully installed cffi-1.16.0 pycparser-2.22 soundfile-0.12.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Asus\\Desktop\\alphablocks\\firday_projects\\generative_ai_learning_env\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai-whisper\n",
      "  Downloading openai-whisper-20231117.tar.gz (798 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: torch in c:\\users\\asus\\desktop\\alphablocks\\firday_projects\\generative_ai_learning_env\\lib\\site-packages (from openai-whisper) (2.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\asus\\desktop\\alphablocks\\firday_projects\\generative_ai_learning_env\\lib\\site-packages (from openai-whisper) (1.26.4)\n",
      "Collecting tiktoken\n",
      "  Using cached tiktoken-0.7.0-cp39-cp39-win_amd64.whl (798 kB)\n",
      "Collecting more-itertools\n",
      "  Downloading more_itertools-10.3.0-py3-none-any.whl (59 kB)\n",
      "Collecting numba\n",
      "  Downloading numba-0.60.0-cp39-cp39-win_amd64.whl (2.7 MB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asus\\desktop\\alphablocks\\firday_projects\\generative_ai_learning_env\\lib\\site-packages (from openai-whisper) (4.66.4)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0\n",
      "  Downloading llvmlite-0.43.0-cp39-cp39-win_amd64.whl (28.1 MB)\n",
      "Collecting regex>=2022.1.18\n",
      "  Using cached regex-2024.7.24-cp39-cp39-win_amd64.whl (269 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\asus\\desktop\\alphablocks\\firday_projects\\generative_ai_learning_env\\lib\\site-packages (from tiktoken->openai-whisper) (2.32.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\desktop\\alphablocks\\firday_projects\\generative_ai_learning_env\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\desktop\\alphablocks\\firday_projects\\generative_ai_learning_env\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\desktop\\alphablocks\\firday_projects\\generative_ai_learning_env\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.7.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\desktop\\alphablocks\\firday_projects\\generative_ai_learning_env\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.7)\n",
      "Requirement already satisfied: sympy in c:\\users\\asus\\desktop\\alphablocks\\firday_projects\\generative_ai_learning_env\\lib\\site-packages (from torch->openai-whisper) (1.13.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\desktop\\alphablocks\\firday_projects\\generative_ai_learning_env\\lib\\site-packages (from torch->openai-whisper) (3.1.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\asus\\desktop\\alphablocks\\firday_projects\\generative_ai_learning_env\\lib\\site-packages (from torch->openai-whisper) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\desktop\\alphablocks\\firday_projects\\generative_ai_learning_env\\lib\\site-packages (from torch->openai-whisper) (3.15.4)\n",
      "Requirement already satisfied: networkx in c:\\users\\asus\\desktop\\alphablocks\\firday_projects\\generative_ai_learning_env\\lib\\site-packages (from torch->openai-whisper) (3.2.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\asus\\desktop\\alphablocks\\firday_projects\\generative_ai_learning_env\\lib\\site-packages (from torch->openai-whisper) (2024.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\desktop\\alphablocks\\firday_projects\\generative_ai_learning_env\\lib\\site-packages (from jinja2->torch->openai-whisper) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\asus\\desktop\\alphablocks\\firday_projects\\generative_ai_learning_env\\lib\\site-packages (from sympy->torch->openai-whisper) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\desktop\\alphablocks\\firday_projects\\generative_ai_learning_env\\lib\\site-packages (from tqdm->openai-whisper) (0.4.6)\n",
      "Building wheels for collected packages: openai-whisper\n",
      "  Building wheel for openai-whisper (PEP 517): started\n",
      "  Building wheel for openai-whisper (PEP 517): finished with status 'done'\n",
      "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=801374 sha256=e891223092ad768192563a4b933b6b35600c7958eaaaf847854e62ed161f20cb\n",
      "  Stored in directory: c:\\users\\asus\\appdata\\local\\pip\\cache\\wheels\\f5\\77\\96\\4bb7b94449a47b726127100ad66bd72cba123fb4d0a8948473\n",
      "Successfully built openai-whisper\n",
      "Installing collected packages: regex, llvmlite, tiktoken, numba, more-itertools, openai-whisper\n",
      "Successfully installed llvmlite-0.43.0 more-itertools-10.3.0 numba-0.60.0 openai-whisper-20231117 regex-2024.7.24 tiktoken-0.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Asus\\Desktop\\alphablocks\\firday_projects\\generative_ai_learning_env\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrating...\n",
      "Okay, go!\n",
      "listening now...\n",
      "Recognizing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\Desktop\\alphablocks\\firday_projects\\generative_ai_learning_env\\lib\\site-packages\\whisper\\__init__.py:146: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hey, what can I do?\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'chatgpt_chain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mlisten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 37\u001b[0m, in \u001b[0;36mlisten\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m     text \u001b[38;5;241m=\u001b[39m unrecognized_speech_text\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(text)\n\u001b[1;32m---> 37\u001b[0m response_text \u001b[38;5;241m=\u001b[39m \u001b[43mchatgpt_chain\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(human_input\u001b[38;5;241m=\u001b[39mtext)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(response_text)\n\u001b[0;32m     39\u001b[0m engine\u001b[38;5;241m.\u001b[39msay(response_text)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'chatgpt_chain' is not defined"
     ]
    }
   ],
   "source": [
    "listen()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generative_ai_learning_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
